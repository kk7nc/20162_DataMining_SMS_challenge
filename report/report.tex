\documentclass[a4paper,12pt]{report}
\usepackage[utf8]{vietnam}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
%\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{cases}
\usepackage{fancybox}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{listings}
\usepackage[nottoc]{tocbibind}
\usepackage{indentfirst}
\usepackage[english]{babel}
\usepackage{float}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}  
\usepackage[left=3cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}
%\lstset{
   %keywords={break,case,catch,continue,else,elseif,end,for,function,
   %   global,if,otherwise,persistent,return,switch,try,while},
%   language = Java,
%   basicstyle=\ttfamily \fontsize{12}{15}\selectfont,   
	% numbers=left,
%   frame=lrtb,
%tabsize=3
%}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=red 
}
\setlength{\parskip}{0.6em}
\addto\captionsenglish{%
 \renewcommand\chaptername{Phần}
 \renewcommand{\contentsname}{Mục lục} 
 \renewcommand{\listtablename}{Danh sách bảng}
 \renewcommand{\listfigurename}{Danh sách hình vẽ}
 \renewcommand{\tablename}{Bảng}
 \renewcommand{\figurename}{Hình}
 \renewcommand{\bibname}{Tài liệu tham khảo}
}

\newtheorem{definition}{Định nghĩa}[chapter]
%\newtheorem{lema}{Bổ đề}[chapter]
%\newtheorem{theorem}{Định lý}[chapter]

\begin{document}
\thispagestyle{empty}
\thisfancypage{
\setlength{\fboxrule}{1pt}
\doublebox}{}

\begin{center}
{\fontsize{16}{19}\fontfamily{cmr}\selectfont TRƯỜNG ĐẠI HỌC BÁCH KHOA HÀ NỘI\\
VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG}\\
\textbf{------------*******---------------}\\[1cm]
\includegraphics[scale=0.13]{hust.jpg}\\[1.3cm]
{\fontsize{32}{43}\fontfamily{cmr}\selectfont BÁO CÁO}\\[0.1cm]
{\fontsize{38}{45}\fontfamily{cmr}\fontseries{b}\selectfont MÔN HỌC}\\[0.2cm]
{\fontsize{20}{24}\fontfamily{phv}\selectfont Khai phá dữ liệu}\\[0.3cm]
{\fontsize{18}{20}\fontfamily{cmr}\selectfont \emph{Đề tài:  Vietnamese SMS Spam Filter Datamining Challenge }}\\[2cm]
\hspace{-5cm}\fontsize{14}{16}\fontfamily{cmr}\selectfont \textbf{Nhóm sinh viên thực hiện:}\\[0.1cm] 
\begin{longtable}{l c c}
Nguyễn Tuấn Đạt & 20130856 & CNTT2.02-K58 \\
Nguyễn Đức Mạnh & 20131518 & CNTT2.04-K58\\
Phan Anh Tú &   20134501 & CNTT2.01-K58\\
\end{longtable}

\hspace{-6cm}\fontsize{14}{16}\fontfamily{cmr}\selectfont \textbf{Giảng viên hướng dẫn:}\\[0.1cm]
\hspace{-2.7cm}\fontsize{14}{16}\fontfamily{cmr}\selectfont TS. Trịnh Anh Phúc \\[3.0cm]
\fontsize{16}{19}\fontfamily{cmr}\selectfont Hà Nội 5--2017
\end{center}
\newpage
\pdfbookmark{\contentsname}{toc}
\tableofcontents
%\listoftables
%\listoffigures

\chapter{Tổng quan}
Trong challenge này, chúng em thử nghiệm qua nhiều phương pháp (SVM, Naive Bayes, KNN, Random forest, Word2Vec,...). Nhưng hai phương pháp cho hiệu quả nhất đó là Naive Bayes và K-nearest neighbors vì vậy trong báo cáo này bọn em sẽ trình bày 2 phương pháp trên.
\section{Bài toán lọc tin nhắn Spam}
Tin nhắn spam là những tin nhắn vô nghĩa mà người sử dụng cảm thấy khó chịu khi nhận nó. \\

Mô hình nhận đầu vào là một tin nhắn văn bản và trả về kết quả dự đoán xem tin nhắn đó có phải là tin rác hoặc không? Độ chính xác của mô hình số lượng tin được dự đoán đúng.
\section{Dữ liệu}
Đầu vào của dữ liệu là tập các tin nhắn đã được chuẩn hóa(bỏ dấu,loại bỏ thông tin thừa...). \\
\begin{itemize}
\item Kích thước dữ liệu 100 tin tập train, 300 tin tập test.
\item Tập train gồm 2 nhãn 1(spam),-1(harm)
\end{itemize}

\section{Biểu diễn vector đầu vào cho bài toán phân loại}
Chúng em đã thử nghiệm nhiều phương pháp biểu diễn một tin nhắn dạng text thành một vector số để máy tính có thể tính toán như: tách từ bởi khoảng trắng, n-gram, word2vec.
\subsection{Tách từ}
Dựa vào tập train đầu vào chúng em xây dựng một từ điển gồm những từ xuất hiện trong tập train bằng cách duyệt qua tất các tin nhắn trong tập train và tách các từ bằng các khoảng trắng. Từ mỗi câu trong tập dữ liệu sẽ được chuyển thành một vector đầu vào cho mô hình học máy. Vector đầu vào sẽ là một vector nhị phân có số chiều bằng số từ của từ điển và thành phần thứ i của vector: 
\begin{itemize}
\item = 1: tức là câu đó có chứa từ i trong tập từ điển
\item = 0: tức la câu đó không có chứa từ i trong tập từ điển
\end{itemize}

Trên thực tế với bộ train ban đầu từ điển bao gồm 910 từ vậy kích thước dữ liệu train đầu vào cho mô hình học máy sẽ là 100x910

\subsection{N-gram}
Duyệt qua tất cả các tin nhắn, mỗi một cụm n ký tự sẽ tạo thành một từ trong từ điển (các từ trong từ điển khác nhau). Tương tự như cách tách từ bởi khoảng trắng, duyệt qua tất cả các tin nhắn và xây dựng một bộ từ điển. Một tin nhắn sẽ được biểu diễn bằng một vector nhị phân có số chiều bằng kích thước của từ điển, cứ mỗi cụm n từ xuất hiện trong tin nhắn dạng text thì tại vị trí của từ đó trong từ điển thành phần của vector biểu diễn tin nhắn có giá trị bằng 1, các thành phần còn lại sẽ có giá trị 0.

\subsection{Word2vec}
Phương pháp này sẽ biểu diễn mỗi từ thành một vector có giá trị là các số thực trong khoảng [0,1]
\subsubsection{Mô hình Continuous Bag of Words Model (CBOW)}
Mô hình này sẽ đoán "\emph{center word}" dựa trên ngữ cảnh của nó trong câu (\emph{context}). Ví dụ cần dự đoán từ "\emph{jumbed}" dựa vào tập các từ \{ "\emph{the}", "\emph{cat}", "\emph{over}", "\emph{puddle}" \}.
\par Đầu tiên cần khởi tạo 2 ma trận tham số $V \in \mathbb{R}^{n \times |D|}$, $U \in \mathbb{R}^{|V| \times n}$. $V$ là ma trận mà cột i là embedded vector $n$ chiều của từ thứ i trong từ điển khi nó được input vào mô hình, $U$ là ma trận mà hàng thứ i là embedded vector $n$ chiều của từ thứ i khi output ra khỏi mô hình.($n$ là số chiều của word vector mong muốn). Mô hình cần học cả hai ma trận $U$ và $V$ để có thể biểu diễn được các từ thành các vector. Để học được hai ma trận $U$, $V$ cần trải qua các bước: 
\begin{enumerate}
\item Tạo các one-hot vector cho các từ trong tập context ($x^{c-m}, ..., x^{c-1}, x^{c+1}, ..., x^{c+m}$) (với $c$ là vị trí của center word trong câu, $m$ là kích thước của context ($C$))
\item Tính các embedded word vector cho mỗi context ($v_{c-m} = Vx^{x-m}, v{c-m+1} = Vx^{c-m+1}, ...., v_{c+m} = Vx^{c+m}$).
\item Tính trung bình các vector $\hat{v} = \frac{v_{c-m} + v_{c-m+1} + .... + v_{c+m}}{2m}$
\item Sinh ra vector đánh giá $z = U\hat{v}$
\item Tính vector đầu ra $\hat{y} = softmax(z)$
\item Tính lỗi của vector đầu ra so với vector của center word thực tế sử dụng hàm cross-entropy
$$H(\hat{y},y) = -\sum_{j=1}^{|V|} y_jlog(\hat{y}_j)$$
\item Update các ma trận $U, V$ để cực tiểu hóa hàm lỗi.
\end{enumerate}


\chapter{Các phương pháp phân loại}
\section{Naive Bayes}
\subsection{Cơ sở lý thuyết}
Mục đích của các phương pháp học máy là học một hàm dựa trên dữ liệu cho trước (tập train) và dùng hàm đó để dự đoán dữ liệu tương lai (trong bài toán phân loại là gán nhãn lớp cho dữ liệu). Phương pháp Naive-Bayes giả thiết dữ liệu của bài toán cần phân loại được sinh ra theo một phân bố xác suất nào đó. 
\subsubsection{Các khái niệm cơ bản vễ xác suất}
Giả sử chúng ta có một thí nghiệm $X$ (ví dụ: tung một con xúc sắc) mà kết quả của nó mang tính ngẫu nhiên.
\begin{itemize}
\item \emph{Không gian các khả năng $S$} là tập hợp tất cả các kết quả có thể xảy ra khi thí nghiệm $X$. Ví dụ $S = \{1,2,3,4,5,6\}$ đối với thí nghiệm tung con xúc sắc.
\item \emph{Sự kiện $E$} là một tập con của không gian các khả năng của thí nghiệm $X$. Ví dụ $E=\{1,3,5\}$ kết quả con xúc sắc đổ ra là một số lẻ.
\item \emph{Không gian các sự kiện W} là không gian (thế giới) mà các kết quả của sự kiện có thể xảy ra. Ví dụ $W$ bao gồm tất cả các lần tung xúc sắc.
\item \emph{Biến ngẫu nhiên A} biểu diễn (diễn đạt) một sự kiện, và có một mức độ về khả năng xảy ra sự kiện này.
\item $P(A)$ là tỉ lệ phần không gian mà trong đó $A$ đúng trên không gian các của tất cả các giá trị có thể xảy ra của A) 
\item \emph{Xác suất đồng thởi (kết hợp) $P(A,B)$} là tỉ lệ của không gian mà trong đó $A$ đúng và $B$ đúng trên toàn bộ không gian của sự kiện $A$ và $B$
\item \emph{Xác suất có điều kiện $P(A|B)$} là tỉ lệ không gian mà trong đó $A$ đúng khi biết $B$ 
$$P(A|B) = \frac{P(A,B)}{P(B)}$$
\item Hai sự kiện $A$ và $B$ được gọi là độc lập về xác suất nếu xác suất của sự kiện $A$ là như nhau đối với tất cả các trường hợp: Khi sự kiện $B$ xảy ra hoặc khi sự kiện $B$ không xảy ra hoặc không có thông tin (không biết gì) về việc xảy ra của sự kiện $B$
\item \emph{Xác suất có điều kiện với nhiều biến $P(A|B,C)$} là xác suất của $A$ khi đã biết $B$ và $C$
\item Hai biến $A$ và $C$ được gọi là độc lập có điều kiện với biến $B$ nếu $P(A|B,C) = P(A|B)$
\item \emph{Quy tắc chuỗi} $P(A_1,A_2,...,A_n) = P(A_1|C).P(A_2|C)...(P(A_n|C)$ nếu $A_1,A_2,...,A_n$ là độc lập có điều kiện với $C$    
\end{itemize}

\subsubsection{Định lý Bayes}
$$P(h|D) = \frac{P(D|h).P(h)}{P(D)}$$
\begin{itemize}
\item $P(h)$: Xác suất của giả thiết $h$
\item $P(D)$: Xác suất của việc quan sát được dữ liệu D.
\item $P(D|h)$: Xác suất (có điều kiện) của việc quan sát được dữ liệu $D$, nếu biết giả thiết $h$ là đúng.
\item $P(h|D)$: Xác suất của giả thiết $h$ là đúng, nếu quan sát được dữ liệu $D$.
\end{itemize}

\subsection{Phân loại Naive-Bayes}
Biểu diễn bài toán phân loại:
\begin{itemize}
\item Một học $D_train$, trong đó mỗi ví dụ học $x$ được biểu diễn là một vector $n$ chiều ($x_1,x_2, ..., x_n$)
\item Một tập xác định các nhãn lớp $C = \{c_1, c_2, ..., c_m\}$
\item Với một ví dụ $z$, thì $z$ sẽ được phân vào lớp nào?
\end{itemize}
\par Mục tiêu: Xác định phân lớp có thể (phù hợp) nhất đối với $z$ tức 
xác định:
$$c = \displaystyle arg\max_{c_i \in C}P(c_i|z) =\displaystyle arg\max_{c_i \in C}P(c_i|z_1,z_2,...,z_n) $$
\par Theo định lý Bayes ta có:
$$c = \displaystyle arg\max_{c_i \in C}\frac{P(z_1,z_2,...,z_n|c_i).P(c_i)}{P(z_1,z_2,...,z_n)}$$
\par Do $P(z_1,z_2,...,z_n)$ là như nhau đối với mọi nhãn lớp $c_i$. Để tìm nhãn lớp có thể nhất đối với $z$ ta cần tìm 
$$c = \displaystyle arg\max_{c_i \in C}P(z_1,z_2,...,z_n|c_i).P(c_i)$$
\par Giả sử các thuộc tính là độc lập có điều kiện đối với các nhãn lớp $c_i$, ta có:
$$P(z_1,z_2,...,z_n|c_i).P(c_i) = \prod_{j=1}^n P(z_j|c_i)$$
\par Như vậy phương pháp phân loại Naive-Bayes sẽ tìm nhãn lớp $c$ có thể nhất đối với $z$ thỏa mãn:
$$c = \displaystyle arg\max_{c_i \in C}P(c_i).\prod_{j=1}^n P(z_j|c_i)$$
\subsection{Giải thuật}
\begin{itemize}
\item Giai đoạn học
\begin{itemize}
\item Tính giá trị xác suất trước $P(c_i)$ đối với mỗi nhãn lớp có thể $c_i \in C$
\item Tính giá trị xác suất xảy ra của mỗi giá trị thuộc tính $x_j$ đối với mỗi nhãn lớp $c_i$ : $P(x_j|c_i)$
\end{itemize}
\item Giai đoạn phân lớp, đối với một ví dụ mới $z$
\begin{itemize}
\item Đối với mỗi phân lớp $c_i \in C$, tính giá trị của biểu thức:
$$P(c_i).\prod_{j=1}^n P(z_j|c_i)$$
\item Xác định phân lớp của $z$ là lớp có thể nhất $c*$
$$c* = \displaystyle arg\max_{c_i \in C}P(c_i).\prod_{j=1}^n P(z_j|c_i)$$   
\end{itemize}
\end{itemize}

\section{K-nearest neighbors (KNN)}
\subsection{Cơ sở lý thuyết}
Nếu như trong phương pháp phân loại Naive-Bayes có giả sử hàm mục tiêu tuân theo một phân bố xác suất nào đó thì trong phương pháp KNN không có giả sử gì về hàm mục tiêu. Ý tưởng chính của phương pháp là trong quá trình học chỉ lưu lại các dữ liệu huấn luyện và việc dự đoán cho một quan sát mới sẽ dựa vào các hàng xóm gần nhất trong tập học. Do đó KNN là một phương pháp phi tham số.
\par Vậy thế nào là hàng xóm gần nhất, dựa vào đâu để xác định hai đối tượng là gần nhau; Và chọn bao nhiêu hàng xóm để có thể dự đoán (gán nhãn) cho một ví dụ mới. Hai thành phần chính của phương pháp phân loại KNN là:
\begin{itemize}
\item Độ đo tương đồng giữa các đối tượng.
\item Số lượng các hàng xóm sẽ dùng vào việc phán đoán (phân loại)
\end{itemize} 

\subsection{Giải thuật}
\begin{itemize}
\item Mỗi ví dụ học $x$ được biểu diễn bởi 2 thành phần:
\begin{itemize}
\item Mô tả của ví dụ $x = (x_1, x_2, x_3, ..., x_n)$ trong đó $x_i \in \mathbb{R}$
\item Nhãn lớp: $c \in C$, với $C$ là tập các nhãn lớp được xác định trước
\end{itemize}
\item Giai đoạn học
\begin{itemize}
\item Đơn giản là lưu lại các ví dụ học trong tập hoc $D$
\end{itemize}
\item Giai đoạn phân lớp: Để phân lớp cho một ví dụ mới $z$
\begin{itemize}
\item Với mỗi ví dụ học $x \in D$, tính khoảng cách giữa $x$ và $z$
\item Xác định tập $NB(z)$ các láng giềng gần nhất của $z$ (Gồm $k$ ví dụ học trong $D$ gần nhất với $z$ tính theo một hàm khoảng cách $d$)
\item Phân $z$ và lớp chiếm số đông trong số các lớp của các ví dụ trong $NB(z)$
\end{itemize}
\end{itemize}

\chapter{Thực nghiệm}
Vì tập dữ liệu khá nhỏ nên phương pháp bọn em sử dụng để đánh giá mô hình là : repeated hold-out. Chúng em thực hiện phương pháp hold out(80 train-20 test) 100 lần sau đó tính trung bình để đánh giá mô hình học máy.\\

Về lựa chọn tham số cho từng phương pháp học máy bọn em sử dụng phương pháp hold-out. Cụ thể sẽ được trình bày trong hai mục dưới đây
\section{Naive Bayes}
Với phương pháp naive-bayes bọn em thực hiện đánh giá trên 3 mô hình:
\begin{itemize}
\item Phương pháp naive bayes đơn thuần
\item Phương pháp naive bayes với tập từ điển các từ có xác suất harm spam cao build từ tập train.
\item Phương pháp naive bayes với tập train được thêm dữ liệu
\end{itemize}
\subsection{Tập từ điển harm spam}
Bọn thực hiện build một tập từ điển harm spam gồm 60 từ được lọc ra từ tập train. Nhưng từ trong từ điển là những từ thoản mãn 2 điều kiện sau:
Xét một từ x gọi CX là  tập câu chứa từ x
\begin{itemize}
\item $P_{CX}['Harm'] or P_{CX}['Spam'] >0.7$
\item $tf(x,d) >0.4$ tần số xuất hiện của từ đó trong 1 văn bản lớn hơn 40 \%
\end{itemize}
Kết quả cụ thể của ba mô hình thể hiện trong bảng sau:
\begin{longtable}{|c|c|}
\hline 
Mô hình & Độ chính xác \\ \hline
naive-bayes normal & 92.9\\ \hline
navie-bayes with dict & 92.1 \\ \hline
navie-bayes more data & 75.375\\ \hline
\end{longtable}


\section{K-nearest neighbors}
Với phương pháp KNN bọn em thực hiện chọn tham số mô hình (k,độ đo). 
\begin{longtable}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Hàm khoảng cách & k=1 &k2& k=3&k=4 & k=5 &k=6 & k=7&k=8 & k=9\\
\hline
euclidean & 0.561&0.577 &0.495 &0.4895&0.485&0.4885&0.4955&0.488&0.485 \\
\hline 
jaccard & 0.9 &0.884& 0.9095 &9215& 0.905 & 0.9165& 0.8995 &0.9115& 0.879 \\ \hline
matching & 0.5655 &0.576& 0.4835 & 0.5015&  0.501 & 0.4945&0.4885 &0.47& 0.489 \\ \hline 
dice &0.8825 &0.8985& 0.904 &0.9205&  0.902 & 0.912& 0.8955 &0.8945& 0.8855 \\ \hline
kulsinski&  0.869 & 0.828& 0.8935&0.8905&  0.878& 0.884& 0.882 & 0.883& 0.875 \\ \hline 
rogerstanimoto&  0.5865& 0.543& 0.483&0.5015& 0.495 & 0.485&0.4995& 0.4845& 0.487 \\ \hline
russellrao&  0.8495&0.786& 0.8695& 0.8205&0.8195 &0.8505  &0.8505& 0.836& 0.8505 \\ \hline 
sokalmichener& 0.5595&0.553& 0.505 & 0.4865 &0.483 &0.474 &0.492& 0.4995 &0.4755 \\ \hline 
sokalsneath& 0.896& 0.8735& 0.919 &0.9125 & 0.9075&0.920& 0.892 & 0.9135& 0.889 \\ \hline
\end{longtable}

Một vài định nghĩa sử dụng cho các hàm khoảng cách dưới đây:
\begin{itemize}
\item N số chiều của vector
\item NTT số chiều mà cả hai giá trị đều bằng 1
\item NTF số chiều mà giá trị của vector 1 bằng 1 vector 2 bằng 0
\item NFT số chiều mà giá trị của vector 2 băng 1 vector 1 bằng 0
\item NFF số chiều mà cả hai giá trị đều bằng 0
\item NNEQ NNEQ=NTF+NFT
\item NNZ NNZ=NTF+NFT+NTT 
\end{itemize}
Định nghĩa các hàm khoảng cách:
\begin{itemize}
\item euclidean : $\sqrt{\sum_1^N (x-y)^2}$
\item “jaccard”	JaccardDistance	NNEQ / NNZ
\item “matching”	MatchingDistance	NNEQ / N
\item “dice”	DiceDistance	NNEQ / (NTT + NNZ)
\item “kulsinski”	KulsinskiDistance	(NNEQ + N - NTT) / (NNEQ + N)\item “rogerstanimoto”	RogersTanimotoDistance	2 * NNEQ / (N + NNEQ)
\item “russellrao”	RussellRaoDistance	NNZ / N
\item “sokalmichener”	SokalMichenerDistance	2 * NNEQ / (N + NNEQ)
\item “sokalsneath”	SokalSneathDistance	NNEQ / (NNEQ + 0.5 * NTT)
\end{itemize}
\section{Kết luận}
Dựa trên kết quả thu được bọn em quyết định lựa chọn 2 mô hình là: 
\begin{itemize}
\item Naive Bayes Normal
\item KNN Jaccard với k=3
\end{itemize}
\begin{thebibliography}{9}
\bibitem{3} \url{http://cs224d.stanford.edu/syllabus.html}
\end{thebibliography}


\end{document}
